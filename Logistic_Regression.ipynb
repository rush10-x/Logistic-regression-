{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Theory Questions**"
      ],
      "metadata": {
        "id": "OkcZ9BbRDndP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "- Logistic Regression is a statistical method used for binary classification problems, where the outcome variable is categorical and typically takes on two possible outcomes (e.g., success/failure, yes/no, 1/0). It models the probability that a given input point belongs to a particular category.\n",
        "- The primary differences between Logistic Regression and Linear Regression are:\n",
        "\n",
        "Output Type:\n",
        "\n",
        "Linear Regression: Predicts a continuous output. The relationship between the independent variables and the dependent variable is modeled as a linear equation.\n",
        "Logistic Regression: Predicts a probability that the output belongs to a particular class, which is then thresholded to make a binary decision.\n",
        "Mathematical Model:\n",
        "\n",
        "Linear Regression: The model is expressed as:\n",
        "Logistic Regression: The model uses the logistic function to map predicted values to probabilities:\n",
        "\n",
        "Q2. What is the mathematical equation of Logistic Regression.\n",
        "- The mathematical equation of Logistic Regression can be expressed as:\n",
        "The mathematical equation of Logistic Regression models the probability that a given input\n",
        "ùë•\n",
        "x belongs to a particular class (usually class 1 in binary classification). The equation is based on the logistic (sigmoid) function applied to a linear combination of the input features.\n",
        "\n",
        "Logistic Regression Equation\n",
        "Let:\n",
        "\n",
        "ùë•\n",
        "=\n",
        "[\n",
        "ùë•\n",
        "1\n",
        ",\n",
        "ùë•\n",
        "2\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùë•\n",
        "ùëõ\n",
        "]\n",
        "x=[x\n",
        "1\n",
        "‚Äã\n",
        " ,x\n",
        "2\n",
        "‚Äã\n",
        " ,‚Ä¶,x\n",
        "n\n",
        "‚Äã\n",
        " ]: input features\n",
        "\n",
        "ùë§\n",
        "=\n",
        "[\n",
        "ùë§\n",
        "1\n",
        ",\n",
        "ùë§\n",
        "2\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùë§\n",
        "ùëõ\n",
        "]\n",
        "w=[w\n",
        "1\n",
        "‚Äã\n",
        " ,w\n",
        "2\n",
        "‚Äã\n",
        " ,‚Ä¶,w\n",
        "n\n",
        "‚Äã\n",
        " ]: model weights\n",
        "\n",
        "ùëè\n",
        "b: bias term (intercept)\n",
        "\n",
        "ùëß\n",
        "=\n",
        "ùë§\n",
        "ùëá\n",
        "ùë•\n",
        "+\n",
        "ùëè\n",
        "z=w\n",
        "T\n",
        " x+b: linear combination\n",
        "\n",
        "Then, the logistic (sigmoid) function is:\n",
        "\n",
        "ùúé\n",
        "(\n",
        "ùëß\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "ùëß\n",
        "œÉ(z)=\n",
        "1+e\n",
        "‚àíz\n",
        "\n",
        "1\n",
        "‚Äã\n",
        "\n",
        "So, the probability that the output\n",
        "ùë¶\n",
        "=\n",
        "1\n",
        "y=1 given input\n",
        "ùë•\n",
        "x is:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë¶\n",
        "=\n",
        "1\n",
        "‚à£\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "(\n",
        "ùë§\n",
        "ùëá\n",
        "ùë•\n",
        "+\n",
        "ùëè\n",
        ")\n",
        "P(y=1‚à£x)=\n",
        "1+e\n",
        "‚àí(w\n",
        "T\n",
        " x+b)\n",
        "\n",
        "1\n",
        "‚Äã\n",
        "\n",
        "Or simply:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë¶\n",
        "=\n",
        "1\n",
        "‚à£\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "ùúé\n",
        "(\n",
        "ùë§\n",
        "ùëá\n",
        "ùë•\n",
        "+\n",
        "ùëè\n",
        ")\n",
        "P(y=1‚à£x)=œÉ(w\n",
        "T\n",
        " x+b)\n",
        "\n",
        "\n",
        "where ( z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n ) and ( \\sigma(z) ) is the sigmoid function.\n",
        "\n",
        "Q3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "- The Sigmoid function is used in Logistic Regression because it maps any real-valued number into the range (0, 1), making it suitable for modeling probabilities. The S-shaped curve of the sigmoid function allows for a smooth transition between the two classes, providing a probabilistic interpretation of the output.\n",
        "\n",
        "Q4. What is the cost function of Logistic Regression.\n",
        "- The cost function for Logistic Regression is based on the concept of maximum likelihood estimation. It is defined as the negative log-likelihood of the predicted probabilities. The cost function ( J(\\beta) ) is given by:\n",
        "\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "( m ) is the number of training examples,\n",
        "( y^{(i)} ) is the actual label,\n",
        "( h_\\beta(x^{(i)}) ) is the predicted probability for the ( i )-th example.\n",
        "\n",
        "Q5. What is Regularization in Logistic Regression? Why is it needed.\n",
        "- Regularization in logistic regression is a technique used to prevent overfitting by discouraging the model from fitting the training data too closely. It does this by adding a penalty term to the loss function that penalizes large coefficients.\n",
        "\n",
        "\n",
        "\n",
        "1. L2 Regularization (Ridge)\n",
        "Adds the sum of squared coefficients to the loss function.\n",
        "\n",
        "Penalizes large weights, encouraging smaller, evenly distributed coefficients.\n",
        "\n",
        "Loss function with L2:\n",
        "\n",
        "ùêΩ\n",
        "(\n",
        "ùë§\n",
        ",\n",
        "ùëè\n",
        ")\n",
        "=\n",
        "‚àí\n",
        "1\n",
        "ùëö\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëö\n",
        "[\n",
        "ùë¶\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "ùë¶\n",
        "^\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùë¶\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùë¶\n",
        "^\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùë§\n",
        "ùëó\n",
        "2\n",
        "J(w,b)=‚àí\n",
        "m\n",
        "1\n",
        "‚Äã\n",
        "  \n",
        "i=1\n",
        "‚àë\n",
        "m\n",
        "‚Äã\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        "  \n",
        "(i)\n",
        " )+(1‚àíy\n",
        "(i)\n",
        " )log(1‚àí\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        "  \n",
        "(i)\n",
        " )]+Œª\n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " w\n",
        "j\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "2. L1 Regularization (Lasso)\n",
        "Adds the sum of absolute values of coefficients.\n",
        "\n",
        "Can shrink some weights to zero, effectively performing feature selection.\n",
        "\n",
        "Loss function with L1:\n",
        "\n",
        "ùêΩ\n",
        "(\n",
        "ùë§\n",
        ",\n",
        "ùëè\n",
        ")\n",
        "=\n",
        "‚àí\n",
        "1\n",
        "ùëö\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëö\n",
        "[\n",
        "ùë¶\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "ùë¶\n",
        "^\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùë¶\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùë¶\n",
        "^\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "‚à£\n",
        "ùë§\n",
        "ùëó\n",
        "‚à£\n",
        "J(w,b)=‚àí\n",
        "m\n",
        "1\n",
        "‚Äã\n",
        "  \n",
        "i=1\n",
        "‚àë\n",
        "m\n",
        "‚Äã\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        "  \n",
        "(i)\n",
        " )+(1‚àíy\n",
        "(i)\n",
        " )log(1‚àí\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        "  \n",
        "(i)\n",
        " )]+Œª\n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " ‚à£w\n",
        "j\n",
        "‚Äã\n",
        " ‚à£\n",
        "3. Elastic Net Regularization\n",
        "Combines both L1 and L2 penalties:\n",
        "\n",
        "ùêΩ\n",
        "(\n",
        "ùë§\n",
        ",\n",
        "ùëè\n",
        ")\n",
        "=\n",
        "LogLoss\n",
        "+\n",
        "ùúÜ\n",
        "1\n",
        "‚àë\n",
        "‚à£\n",
        "ùë§\n",
        "ùëó\n",
        "‚à£\n",
        "+\n",
        "ùúÜ\n",
        "2\n",
        "‚àë\n",
        "ùë§\n",
        "ùëó\n",
        "2\n",
        "J(w,b)=LogLoss+Œª\n",
        "1\n",
        "‚Äã\n",
        " ‚àë‚à£w\n",
        "j\n",
        "‚Äã\n",
        " ‚à£+Œª\n",
        "2\n",
        "‚Äã\n",
        " ‚àëw\n",
        "j\n",
        "2\n",
        "‚Äã\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "When a logistic regression model has:\n",
        "\n",
        "Too many features\n",
        "\n",
        "Noisy data\n",
        "\n",
        "Small training data\n",
        "\n",
        "‚Ä¶it might overfit ‚Äî meaning it performs well on training data but poorly on unseen data. Overfitting happens when the model becomes too complex and starts capturing noise or random fluctuations instead of the underlying pattern.\n",
        "\n",
        "Regularization helps to:\n",
        "\n",
        "Reduce model complexity\n",
        "\n",
        "Improve generalization\n",
        "\n",
        "Make the model more robust to noise"
      ],
      "metadata": {
        "id": "QCJixhK_D3y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Difference between Lasso, Ridge, and Elastic Net Regression\n",
        "\n",
        "- Lasso Regression (L1 Regularization):\n",
        "\n",
        "Adds a penalty equal to the absolute value of the coefficients.\n",
        "Can shrink some coefficients to exactly zero, effectively performing variable selection.\n",
        "Best suited for situations where only a few predictors are significant.\n",
        "Ridge Regression (L2 Regularization):\n",
        "\n",
        "Adds a penalty equal to the square of the coefficients.\n",
        "Shrinks coefficients but does not set any to zero, retaining all features in the model.\n",
        "Ideal for datasets with many correlated predictors, as it stabilizes coefficient estimates.\n",
        "Elastic Net Regression:\n",
        "\n",
        "Combines both L1 and L2 penalties.\n",
        "\n",
        "Balances the benefits of Lasso and Ridge, making it effective for datasets with correlated features.\n",
        "\n",
        "Useful when the number of predictors is greater than the number of observations.\n",
        "\n",
        "Q7. When to Use Elastic Net Instead of Lasso or Ridge\n",
        "\n",
        "- Correlated Features: Use Elastic Net when there are many correlated features, as it can select groups of variables together.\n",
        "\n",
        "High-Dimensional Data: It is beneficial when the number of predictors exceeds the number of observations, addressing issues of overfitting.\n",
        "\n",
        "Feature Selection and Stability: Elastic Net provides a more stable solution than Lasso alone, which may randomly select one feature from a group while ignoring others.\n",
        "\n",
        "Q8. Impact of the Regularization Parameter (Œª) in Logistic Regression\n",
        "\n",
        "- Control Overfitting: The regularization parameter (Œª) helps prevent overfitting by penalizing large coefficients.\n",
        "\n",
        "Model Complexity: A larger Œª increases the penalty, leading to simpler models with fewer non-zero coefficients, which can improve generalization on unseen data.\n",
        "\n",
        "Bias-Variance Trade-off: Adjusting Œª allows for a balance between bias and variance, where higher values may increase bias but reduce variance.\n",
        "\n",
        "Q9. Key Assumptions of Logistic Regression\n",
        "\n",
        "- Independence of Observations: The observations should be independent of each other.\n",
        "\n",
        "Linearity of the Logit: The relationship between the independent variables and the log odds of the dependent variable should be linear.\n",
        "\n",
        "Absence of Multicollinearity: Predictors should not be highly correlated with each other.\n",
        "\n",
        "Binary Outcome: The dependent variable should be binary, representing two classes.\n",
        "\n",
        "Q10. Alternatives to Logistic Regression for Classification Tasks\n",
        "\n",
        "- Decision Trees: Useful for capturing non-linear relationships and interactions between features.\n",
        "\n",
        "Random Forests: An ensemble method that improves accuracy and robustness by combining multiple decision trees.\n",
        "\n",
        "Support Vector Machines (SVM): Effective for high-dimensional spaces and can model non-linear boundaries using kernel functions.\n",
        "\n",
        "Gradient Boosting Machines: Another ensemble technique that builds models sequentially to correct errors made by previous models, often yielding high accuracy."
      ],
      "metadata": {
        "id": "844t7J_2GImX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What are Classification Evaluation Metrics?\n",
        "\n",
        "- Accuracy: The ratio of correctly predicted instances to the total instances.\n",
        "\n",
        "Precision: The ratio of true positive predictions to the total predicted positives, indicating the quality of positive predictions.\n",
        "\n",
        "Recall (Sensitivity): The ratio of true positive predictions to the actual positives, measuring the model's ability to identify all relevant instances.\n",
        "\n",
        "F1-Score: The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
        "\n",
        "AUC-ROC: The area under the receiver operating characteristic curve, representing the model's ability to distinguish between classes across various thresholds.\n",
        "\n",
        "Q12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "- Bias Towards Majority Class: The model may predict the majority class more often, leading to high accuracy but poor performance on the minority class.\n",
        "Misleading Metrics: Standard accuracy can be misleading; a model may appear effective while failing to predict the minority class correctly.\n",
        "Need for Resampling: Techniques like oversampling the minority class or undersampling the majority class can help balance the dataset and improve model performance.\n",
        "Use of Class Weights: Assigning higher weights to the minority class can help the model pay more attention to it during training.\n",
        "Q13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "- Definition: The process of optimizing hyperparameters to improve model performance.\n",
        "Common Hyperparameters:\n",
        "Solver: The algorithm used for optimization (e.g., 'lbfgs', 'liblinear').\n",
        "Penalty: The type of regularization applied (e.g., 'l1', 'l2').\n",
        "C: The regularization strength, where smaller values indicate stronger regularization.\n",
        "Techniques: Methods like GridSearchCV or RandomizedSearchCV are used to systematically explore combinations of hyperparameters and identify the best performing set.\n",
        "Q14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "- liblinear: Best for small datasets; supports L1 and L2 penalties.\n",
        "newton-cg: Suitable for small to medium datasets; works well for multiclass problems.\n",
        "lbfgs: A good default choice for small to medium datasets; efficient in terms of memory.\n",
        "sag: Faster for large datasets when both samples and features are large.\n",
        "saga: Ideal for sparse datasets and large datasets; supports L1 and elastic net penalties.\n",
        "Recommendation: Use 'lbfgs' as a default for most cases, but consider 'saga' for large or sparse datasets.\n",
        "\n",
        "Q15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "- One-vs-Rest (OvR): This approach involves training a separate binary classifier for each class, treating the class as the positive class and all others as negative.\n",
        "Multinomial Logistic Regression: A direct extension that models the probabilities of multiple classes simultaneously, allowing for more complex relationships between classes.\n",
        "Softmax Function: Used in multinomial logistic regression to generalize the logistic function for multiple classes, providing a probability distribution across all classes."
      ],
      "metadata": {
        "id": "R3LjK1MwGViG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. Advantages and Disadvantages of Logistic Regression\n",
        "Advantages:\n",
        "\n",
        "- Simplicity and Interpretability: Logistic regression is easy to understand and interpret. The coefficients can be directly interpreted as the change in the log-odds of the outcome for a one-unit change in the predictor.\n",
        "Efficiency: It is computationally efficient and can be trained quickly, even on large datasets.\n",
        "Probabilistic Output: It provides probabilities for class membership, which can be useful for decision-making.\n",
        "Less Prone to Overfitting: With fewer parameters compared to more complex models, logistic regression is less likely to overfit, especially with a small number of observations.\n",
        "Works Well with Linearly Separable Data: It performs well when the classes are linearly separable.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "- Linearity Assumption: Logistic regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable, which may not hold in all cases.\n",
        "Sensitive to Outliers: Outliers can have a significant impact on the model's performance and can skew the results.\n",
        "Limited to Binary Outcomes: While it can be extended to multiclass problems, its primary application is for binary classification.\n",
        "Requires Large Sample Sizes: For reliable estimates, logistic regression typically requires a larger sample size, especially when the number of predictors is high.\n",
        "\n",
        "\n",
        "Q17. Use Cases of Logistic Regression\n",
        "- Medical Diagnosis: Predicting the presence or absence of a disease based on various health indicators (e.g., predicting whether a patient has diabetes based on glucose levels, age, and BMI).\n",
        "Credit Scoring: Assessing the likelihood of a borrower defaulting on a loan based on their financial history and demographic information.\n",
        "Marketing Response: Predicting whether a customer will respond to a marketing campaign based on their previous behavior and demographic data.\n",
        "Spam Detection: Classifying emails as spam or not spam based on features extracted from the email content.\n",
        "Customer Churn Prediction: Estimating the probability that a customer will stop using a service based on their usage patterns and demographics.\n",
        "\n",
        "Q18. Difference Between Softmax Regression and Logistic Regression\n",
        "- Output Type:\n",
        "\n",
        "Logistic Regression: Used for binary classification problems, providing a single probability output for one class (e.g., class 1) and implicitly the probability for the other class (e.g., class 0).\n",
        "Softmax Regression: Used for multiclass classification problems, providing a probability distribution across multiple classes, where the sum of probabilities for all classes equals 1.\n",
        "\n",
        "Mathematical Formulation:\n",
        "\n",
        "Logistic Regression: Uses the logistic function to model the probability of the positive class:\n",
        "\n",
        "\n",
        "\n",
        "Softmax Regression: Uses the softmax function to model the probabilities for multiple classes:\n",
        "\n",
        "\n",
        "\n",
        "Q19. Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification\n",
        "- One-vs-Rest (OvR):\n",
        "\n",
        "Suitable when the number of classes is large, as it trains a separate binary classifier for each class.\n",
        "Can be more interpretable since each classifier focuses on distinguishing one class from all others.\n",
        "May be preferred when classes are imbalanced.\n",
        "Softmax Regression:\n",
        "\n",
        "More efficient for problems with a smaller number of classes, as it computes probabilities for all classes in a single model.\n",
        "Provides a natural probabilistic interpretation across multiple classes.\n",
        "May perform better when classes are not mutually exclusive or when there is a need for a joint probability distribution.\n",
        "\n",
        "\n",
        "Q20. Interpreting Coefficients in Logistic Regression\n",
        "- In logistic regression, the coefficients represent the change in the log-odds of the dependent variable for a one-unit increase in the predictor variable, holding all other variables constant. Specifically:\n",
        "\n",
        "If\n",
        " is the coefficient for predictor\n",
        ", then the interpretation is:\n",
        "\n",
        "The odds ratio can be calculated as\n",
        ", which indicates how the odds of the outcome change with a one-unit increase in\n",
        ":\n",
        "If\n",
        ", the odds of the outcome increase as $X"
      ],
      "metadata": {
        "id": "FD7gMp_oG5rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "20Y6iIJWHr1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "T3k16iJnH3J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "1uQp_cpgKP5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Use binary classification for simplicity (class 0 vs. not class 0)\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')  # liblinear supports L2 for binary classification\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "print(\"Model Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "id": "4etgWFnUKnwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with Elastic Net\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0, max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "Gk0GiPeULFHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'C\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression for multiclass using One-vs-Rest\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "1Gt-MY2MLa1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6.  Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both l1 and l2\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and score\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy on Test Set:\", grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "Yat7sCMNLgO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load a sample dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Print average accuracy\n",
        "print(\"Average Accuracy (Stratified K-Fold): {:.2f}%\".format(np.mean(accuracies) * 100))\n"
      ],
      "metadata": {
        "id": "HyleDcZvMJv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV file (update path as needed)\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy on Test Data: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "id": "iv2nwFo0MZhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_dist = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],  # l1 may not work with all solvers\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "lJDvPx06MkLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define OvO Logistic Regression\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"OvO Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "B_eGjr24MtwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "# Step 1: Create synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Step 6: Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Print accuracy\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xhEopshFM2C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Step 1: Create synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate Precision, Recall, F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Display the results\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F1 Score:  {f1:.2f}\")\n",
        "\n",
        "# Optional: Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "ciZXt3u-M-J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Generate imbalanced binary classification data\n",
        "X, y = make_classification(n_samples=10000, n_features=10, n_informative=6,\n",
        "                           n_redundant=2, n_clusters_per_class=1,\n",
        "                           weights=[0.90, 0.10], flip_y=0, random_state=42)\n",
        "\n",
        "# Display class distribution\n",
        "print(\"Class distribution:\")\n",
        "print(pd.Series(y).value_counts())\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train logistic regression without class weights\n",
        "model_no_weights = LogisticRegression(max_iter=1000)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "print(\"\\nWithout Class Weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_no_weights))\n",
        "\n",
        "# 4. Train logistic regression with class_weight='balanced'\n",
        "model_balanced = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model_balanced.fit(X_train, y_train)\n",
        "y_pred_balanced = model_balanced.predict(X_test)\n",
        "\n",
        "print(\"\\nWith Class Weights (balanced):\")\n",
        "print(classification_report(y_test, y_pred_balanced))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_balanced))\n",
        "\n",
        "# Optional: Plot confusion matrix heatmap\n",
        "def plot_confusion(cm, title):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion(confusion_matrix(y_test, y_pred_no_weights), \"Without Class Weights\")\n",
        "plot_confusion(confusion_matrix(y_test, y_pred_balanced), \"With Class Weights (Balanced)\")\n"
      ],
      "metadata": {
        "id": "h6awWUEdNDUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Select relevant features and target\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "target = 'survived'\n",
        "\n",
        "df = titanic[features + [target]].copy()\n",
        "\n",
        "# Handle missing values\n",
        "df['age'].fillna(df['age'].median(), inplace=True)\n",
        "df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in ['sex', 'embarked']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "lqCdSRYgNJTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with standard scaling: {acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "DHl5TkuzN7yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "\n",
        " from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "print(f\"ROC-AUC score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "hped1DEUOk8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with C=0.5 (inverse of regularization strength)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "6x0TDk1YOrDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q18.Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Pair feature names with their coefficients\n",
        "feature_importance = list(zip(feature_names, coefficients))\n",
        "\n",
        "# Sort features by absolute coefficient value (importance)\n",
        "feature_importance = sorted(feature_importance, key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "print(\"Feature importance based on coefficients:\")\n",
        "for feature, coef in feature_importance:\n",
        "    print(f\"{feature}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "id": "5_ksKZ0GOxHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa Score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate with Cohen's Kappa\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "BVfQRkGjO3qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision, recall and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Calculate average precision score\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(recall, precision, label=f'Precision-Recall curve (AP={avg_precision:.4f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Logistic Regression')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Cv9yeZyvPAfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracies = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    # saga requires multi_class='multinomial' and max_iter increase for iris dataset\n",
        "    if solver == 'saga':\n",
        "        model = LogisticRegression(solver=solver, max_iter=5000, multi_class='multinomial')\n",
        "    else:\n",
        "        model = LogisticRegression(solver=solver, max_iter=5000)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies[solver] = acc\n",
        "\n",
        "for solver, acc in accuracies.items():\n",
        "    print(f\"Solver: {solver}, Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "2mw9_cPvPXTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n"
      ],
      "metadata": {
        "id": "vPmA0jBgQE8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=5000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize data\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression on standardized data\n",
        "model_std = LogisticRegression(max_iter=5000)\n",
        "model_std.fit(X_train_std, y_train)\n",
        "y_pred_std = model_std.predict(X_test_std)\n",
        "acc_std = accuracy_score(y_test, y_pred_std)\n",
        "\n",
        "print(f\"Accuracy on raw data: {acc_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {acc_std:.4f}\")\n"
      ],
      "metadata": {
        "id": "noOiAvhRQLca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression with cross-validation to find best C\n",
        "model_cv = LogisticRegressionCV(cv=5, max_iter=5000, scoring='accuracy', multi_class='auto', random_state=42)\n",
        "model_cv.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Optimal C found: {model_cv.C_[0]:.4f}\")\n",
        "\n",
        "y_pred = model_cv.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with optimal C: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "aZwttQPGQTac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q25.  Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions\n",
        "\n",
        "import joblib\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "\n",
        "# Load model\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "\n",
        "# Make predictions\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "print(f\"Predictions: {y_pred}\")\n"
      ],
      "metadata": {
        "id": "4Gg8vifgQacM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}